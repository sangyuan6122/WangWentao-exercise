Thinking1	XGBoost与GBDT的区别是什么？
GBDT是一种迭代的决策树算法，该算法由多个决策树组成。xgboost它是一个大规模、分布式的通用Gradient Boosting（GBDT）库，它在Gradient Boosting框架下实现了GBDT和一些广义的线性机器学习算法
1)XBG还支持线性分类器,传统GBDT以CART作为基分类器，xgboost还支持线性分类器;
2)传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数;
3)xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance trade-off角度来讲，
  正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性
4)XGB的权重衰减,xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。
  实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点;
5)XGB支持列抽样,xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，
  xgboost可以自动学习出它的分裂方向;
6)xgboost工具支持并行
7)可并行的近似直方图算法,树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。
  当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点

Thinking2	举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何... 请分享到课程微信群中）
kaggle上员工离职率预测，步骤大致如下:
1)数据探索,观察各个变量的数据结构及主要描述统计量
2)特征选择、处理，主要对特征分类、特征选择
3)建立模型，首先设置交叉验证参数，然后分层抽样、决策树进行建模预测、朴素贝叶斯进行建模预测
4)预测，决策树模型的AUC=0.93，AUC值越大，则模型越好，决策树模型优于朴素贝叶斯模型

Thinking3	请你思考，在你的工作中，需要构建哪些特征（比如用户画像，item特征...），这些特征都包括哪些维度（鼓励分享到微信群中，进行交流）
用户画像,主要从三个特征：基本属性、行为属性、统计属性
其中行为属性包含时间维度、空间维度等，统计属性包含简单统计量、复杂分析值等

Action1	"男女声音识别
见exercise06/voice.py

